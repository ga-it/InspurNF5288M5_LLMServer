# /opt/lmdeploy-v100-debian-testing.env
# Hugging Face configuration
HUGGING_FACE_HUB_TOKEN=hf_XXXXXX

# Model configuration - Update to your actual path
MODEL_PATH=Qwen/Qwen3-30B-A3B
# Or if in HF cache:
# MODEL_PATH=/data/huggingface/hub/models--Qwen--Qwen3-30B/snapshots/[HASH]/

# Parallelism configuration - 8 GPUs for 30B model
TENSOR_PARALLEL_SIZE=8
PIPELINE_PARALLEL_SIZE=1

# Memory and cache settings - Conservative for 30B
CACHE_MAX_ENTRY=0.8
SESSION_LEN=38912
MAX_BATCH_SIZE=16

# Additional optimisations for long context
CACHE_BLOCK_SEQ_LEN=128

# Quantisation (0=no quant, 4=int4, 8=int8)
# Consider int8 quantisation if memory is tight
QUANT_POLICY=0

# Performance tuning
GPU_MEMORY_UTILIZATION=0.90
BLOCK_SIZE=64

# Disable experimental features
HF_HUB_ENABLE_HF_TRANSFER=0
HF_HUB_DISABLE_EXPERIMENTAL_WARNING=1
